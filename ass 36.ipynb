{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f54bbd7",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c3968",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a particular value for a variable or feature in a certain observation or record. This absence can occur for various reasons, such as data entry errors, equipment malfunctions, or intentional omissions.\n",
    "\n",
    "Handling missing values is essential for several reasons:\n",
    "\n",
    "1. **Avoiding Biased Analysis**\n",
    "\n",
    "2. **Maintaining Data Integrity**\n",
    "3. **Improving Model Performance** \n",
    "\n",
    "4. **Enhancing Data Quality**\n",
    "5. **Complying with Model Requirements**\n",
    "\n",
    "Some algorithms that are not affected by missing values or can handle them inherently include:\n",
    "\n",
    "1. **Decision Trees**\n",
    "2. **Random Forest**\n",
    "3. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "4. **Naive Bayes**\n",
    "5. **Principal Component Analysis (PCA)**\n",
    "\n",
    "It's important to note that while these algorithms can handle missing values to some extent, it is still advisable to preprocess and impute missing data appropriately for the best results in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d4512",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bea13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "# Removing Rows with Missing Values:\n",
    "# This involves removing entire rows that contain missing values.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(df_cleaned)\n",
    "\n",
    "\n",
    "# Imputation with Mean, Median, or Mode:\n",
    "# Filling missing values with the mean, median, or mode of the respective column.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "df_imputed = df.fillna(df.mean())\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912db44",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d52055",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the distribution of classes is not equal, meaning that some classes have significantly fewer instances than others. In other words, the dataset is skewed, and there is a disproportionate representation of different classes.\n",
    "\n",
    "For example, consider a binary classification problem where you are predicting whether an email is spam or not. If you have 95% non-spam emails and only 5% spam emails, the data is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several issues:\n",
    "\n",
    "1. **Biased Model Performance:** Machine learning models trained on imbalanced data may become biased towards the majority class. In the example mentioned above, a model might become overly focused on predicting the majority class (non-spam), neglecting the minority class (spam).\n",
    "\n",
    "2. **Poor Generalization:** Imbalanced datasets can result in models that generalize poorly to new, unseen data, especially for the minority class. The model may perform well on the majority class but poorly on the minority class.\n",
    "\n",
    "3. **Misleading Evaluation Metrics:** Traditional accuracy is not a reliable metric when dealing with imbalanced data. A model could achieve high accuracy by simply predicting the majority class, but it might fail to identify instances of the minority class. Other evaluation metrics like precision, recall, F1-score, and area under the receiver operating characteristic (ROC) curve are more informative in such cases.\n",
    "\n",
    "4. **Model Sensitivity:** Imbalanced data can make models sensitive to changes in the distribution of classes. A slight imbalance shift or the introduction of new instances from the minority class may significantly impact model performance.\n",
    "\n",
    "To handle imbalanced data, several techniques can be employed:\n",
    "\n",
    "1. **Resampling:** This involves either oversampling the minority class, undersampling the majority class, or a combination of both to achieve a more balanced distribution.\n",
    "\n",
    "2. **Synthetic Data Generation:** Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic instances of the minority class.\n",
    "\n",
    "3. **Cost-sensitive Learning:** Assigning different misclassification costs to different classes can be effective in guiding the model to pay more attention to the minority class.\n",
    "\n",
    "4. **Ensemble Methods:** Ensemble methods like Random Forest and Gradient Boosting can handle imbalanced data well, especially when combined with resampling techniques.\n",
    "\n",
    "5. **Anomaly Detection Techniques:** If the minority class is considered an anomaly, anomaly detection methods can be employed to identify and handle it separately.\n",
    "\n",
    "It's important to choose the appropriate technique based on the specific characteristics of the dataset and the goals of the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cce2f3",
   "metadata": {},
   "source": [
    "\n",
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ab578",
   "metadata": {},
   "source": [
    "**Up-sampling** and **down-sampling** are two common techniques used to address imbalanced datasets by adjusting the balance between the minority and majority classes.\n",
    "\n",
    "1. **Up-sampling:**\n",
    "   - **Definition:** Up-sampling involves increasing the number of instances in the minority class by randomly replicating existing instances or generating synthetic examples.\n",
    "   - **Example:** Suppose you have a dataset with two classes, Class A and Class B, and Class B is the minority class. In up-sampling, you might randomly duplicate instances from Class B or use methods like SMOTE to generate synthetic examples for Class B.\n",
    "\n",
    "  \n",
    "\n",
    "2. **Down-sampling:**\n",
    "   - **Definition:** Down-sampling involves reducing the number of instances in the majority class by randomly removing instances or using systematic sampling.\n",
    "   - **Example:** Using the same dataset as before, in down-sampling, you might randomly remove instances from the majority class or use systematic sampling to create a more balanced dataset.\n",
    "\n",
    "\n",
    "**When to use Up-sampling and Down-sampling:**\n",
    "\n",
    "- **Up-sampling:**\n",
    "  - Use up-sampling when the amount of data in the minority class is insufficient for the model to learn effectively.\n",
    "  - Up-sampling can be beneficial when the minority class is important, and the goal is to avoid its neglect by the model.\n",
    "\n",
    "- **Down-sampling:**\n",
    "  - Use down-sampling when you have a large amount of data, and the model is biased towards the majority class.\n",
    "  - Down-sampling can be suitable when the majority class is overrepresented, and the goal is to create a more balanced dataset without introducing synthetic instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27d1f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1 Class\n",
      "3         4     B\n",
      "4         5     B\n",
      "5         6     B\n",
      "6         7     B\n",
      "7         8     B\n",
      "8         9     B\n",
      "2         3     A\n",
      "0         1     A\n",
      "2         3     A\n",
      "2         3     A\n",
      "0         1     A\n",
      "0         1     A\n",
      "   Feature1 Class\n",
      "5         6     B\n",
      "8         9     B\n",
      "6         7     B\n",
      "4         5     B\n",
      "0         1     A\n",
      "1         2     A\n",
      "2         3     A\n",
      "3         4     A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "   import pandas as pd\n",
    "   from sklearn.utils import resample\n",
    "\n",
    "   # Sample DataFrame with imbalanced data\n",
    "   data = pd.DataFrame({\n",
    "       'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "       'Class': ['A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B']\n",
    "   })\n",
    "\n",
    "   # Separate minority and majority classes\n",
    "   minority_class = data[data['Class'] == 'A']\n",
    "   majority_class = data[data['Class'] == 'B']\n",
    "\n",
    "   # Up-sample minority class\n",
    "   minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "\n",
    "   # Combine majority class with up-sampled minority class\n",
    "   upsampled_data = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "   print(upsampled_data)\n",
    "  \n",
    "    \n",
    "    \n",
    "   import pandas as pd\n",
    "   from sklearn.utils import resample\n",
    "\n",
    "   # Sample DataFrame with imbalanced data\n",
    "   data = pd.DataFrame({\n",
    "       'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "       'Class': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n",
    "   })\n",
    "\n",
    "   # Separate minority and majority classes\n",
    "   minority_class = data[data['Class'] == 'A']\n",
    "   majority_class = data[data['Class'] == 'B']\n",
    "\n",
    "   # Down-sample majority class\n",
    "   majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "   # Combine down-sampled majority class with minority class\n",
    "   downsampled_data = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "   print(downsampled_data)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46776e57",
   "metadata": {},
   "source": [
    "\n",
    "### Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1ae07",
   "metadata": {},
   "source": [
    "**Data augmentation** is a technique commonly used in machine learning, especially in the context of image and text data, to artificially increase the size of a dataset by applying various transformations to the existing data. The goal is to introduce diversity and variability in the dataset, helping the model generalize better to different scenarios. Data augmentation is particularly useful when working with limited amounts of labeled data.\n",
    "\n",
    "For example, in image classification, data augmentation might involve rotating, flipping, zooming, or shifting images. In text data, augmentation could include adding synonyms, changing word order, or introducing small variations to the text.\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique):**\n",
    "\n",
    "SMOTE is a specific data augmentation technique designed to address imbalanced datasets, where the number of instances in the minority class is significantly lower than in the majority class. Rather than simply replicating existing instances (as in traditional up-sampling), SMOTE generates synthetic examples for the minority class.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "1. **Select a Minority Instance:** For each instance in the minority class, SMOTE selects a data point.\n",
    "\n",
    "2. **Find Nearest Neighbors:** SMOTE identifies k-nearest neighbors (typically k=5) for the selected instance within the minority class.\n",
    "\n",
    "3. **Generate Synthetic Instances:** SMOTE generates synthetic instances by interpolating between the selected instance and its k-nearest neighbors. This is done by creating new instances along the line segments connecting the selected instance to its neighbors.\n",
    "\n",
    "4. **Repeat for Desired Amount:** The process is repeated until the desired balance between the minority and majority classes is achieved.\n",
    "\n",
    "SMOTE effectively addresses the class imbalance problem by introducing synthetic instances, thereby allowing the model to better capture the characteristics of the minority class.\n",
    "\n",
    "\n",
    "In this example, `make_classification` is used to create a synthetic imbalanced dataset, and SMOTE is applied to balance the class distribution. The before-and-after class distributions demonstrate the effectiveness of SMOTE in addressing class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "726d79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class Distribution:\n",
      "1    500\n",
      "0    500\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Class Distribution after SMOTE:\n",
      "1    500\n",
      "0    500\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Generate a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_classes=2,\n",
    "                           n_features=20, \n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for demonstration\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(X.shape[1])])\n",
    "df['Class'] = y\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert to DataFrame for demonstration\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=[f\"Feature_{i}\" for i in range(X_resampled.shape[1])])\n",
    "df_resampled['Class'] = y_resampled\n",
    "\n",
    "print(\"Original Class Distribution:\")\n",
    "print(df['Class'].value_counts())\n",
    "\n",
    "print(\"\\nClass Distribution after SMOTE:\")\n",
    "print(df_resampled['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a1028",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bad733",
   "metadata": {},
   "source": [
    "**Outliers** in a dataset are data points that significantly deviate from the overall pattern of the data. These points lie at an abnormal distance from other values, potentially affecting the statistical analysis and interpretation of the dataset. Outliers can occur due to various reasons, such as measurement errors, data entry mistakes, or genuine extreme values in the underlying distribution.\n",
    "\n",
    "Here are some characteristics of outliers:\n",
    "\n",
    "1. **Unusual Values:** Outliers are data points that differ significantly from the majority of the data.\n",
    "  \n",
    "2. **Impact on Statistics:** Outliers can heavily influence summary statistics, such as the mean and standard deviation, leading to misleading interpretations.\n",
    "\n",
    "3. **Skewing Distributions:** Outliers can distort the distribution of the data, making it appear skewed or non-normally distributed.\n",
    "\n",
    "4. **Affecting Model Performance:** Outliers can have a substantial impact on machine learning models, particularly those sensitive to the scale and distribution of the data.\n",
    "\n",
    "It is essential to handle outliers for several reasons:\n",
    "\n",
    "1. **Statistical Analysis:** Outliers can distort the results of statistical analyses, leading to inaccurate conclusions about the data distribution and relationships.\n",
    "\n",
    "2. **Model Performance:** Outliers can negatively affect the performance of machine learning models. Some models, such as linear regression, can be sensitive to outliers and may produce biased predictions.\n",
    "\n",
    "3. **Data Visualization:** Outliers can make it challenging to visualize and interpret data accurately. Plots and charts may not accurately represent the overall trends in the presence of outliers.\n",
    "\n",
    "4. **Risk of Misinterpretation:** Ignoring outliers can result in misinterpretation of data, leading to incorrect insights and decisions.\n",
    "\n",
    "Methods for handling outliers include:\n",
    "\n",
    "1. **Removing Outliers:** Identify and remove data points that are considered outliers based on statistical methods or domain knowledge.\n",
    "\n",
    "2. **Transforming Data:** Apply mathematical transformations (e.g., logarithmic transformation) to reduce the impact of outliers.\n",
    "\n",
    "3. **Imputing Values:** Replace outlier values with more reasonable values based on interpolation, extrapolation, or other imputation methods.\n",
    "\n",
    "4. **Binning:** Grouping or binning values can help mitigate the impact of extreme values.\n",
    "\n",
    "5. **Robust Statistical Measures:** Use robust statistical measures such as the median and interquartile range (IQR) instead of the mean and standard deviation, which are sensitive to outliers.\n",
    "\n",
    "Handling outliers is context-dependent, and the choice of method depends on the nature of the data and the specific goals of the analysis or modeling task. It's important to carefully consider the impact of outliers and choose an appropriate strategy based on the characteristics of the dataset and the objectives of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfa724",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0ee9975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "# Removing Rows with Missing Values:\n",
    "# This involves removing entire rows that contain missing values.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(df_cleaned)\n",
    "\n",
    "\n",
    "# Imputation with Mean, Median, or Mode:\n",
    "# Filling missing values with the mean, median, or mode of the respective column.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "df_imputed = df.fillna(df.mean())\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33317623",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47908d",
   "metadata": {},
   "source": [
    "When dealing with missing data in a large dataset, it's crucial to understand whether the missingness is random or if there is a pattern to it. This information helps in selecting appropriate strategies for handling the missing values. Here are some strategies to determine if the missing data is missing at random (MAR) or if there is a pattern:\n",
    "\n",
    "1. **Descriptive Statistics:**\n",
    "   - **Strategy:** Calculate summary statistics for the missing and non-missing data.\n",
    "   - **Explanation:** Compare the means, medians, or other summary measures between the groups with missing data and those without. Significant differences may suggest a non-random pattern.\n",
    "\n",
    "2. **Correlation Analysis:**\n",
    "   - **Strategy:** Examine the correlation between the missingness of one variable and the presence of other variables.\n",
    "   - **Explanation:** If the missingness of a variable is significantly correlated with the presence or absence of other variables, it may indicate a pattern.\n",
    "\n",
    "3. **Heatmap or Missingness Matrix:**\n",
    "   - **Strategy:** Create a heatmap or missingness matrix to visualize the pattern of missing values.\n",
    "   - **Explanation:** The visual representation can highlight clusters or patterns of missingness across variables. If missing values are scattered randomly, it might suggest MAR.\n",
    "\n",
    "4. **Missingness Tests:**\n",
    "   - **Strategy:** Conduct statistical tests for missingness patterns.\n",
    "   - **Explanation:** Statistical tests, such as Little's MCAR test or other missing data tests, can help assess whether the missing data follows a specific pattern.\n",
    "\n",
    "5. **Time Series Analysis:**\n",
    "   - **Strategy:** Examine if there is a temporal pattern in the missing data.\n",
    "   - **Explanation:** If missing values occur more frequently at certain times or in certain time periods, it may suggest a non-random pattern.\n",
    "\n",
    "6. **Domain Knowledge:**\n",
    "   - **Strategy:** Leverage domain knowledge to understand the context of missing values.\n",
    "   - **Explanation:** Understanding the data generation process and the reasons for missingness can provide insights into whether the missing data is random or systematic.\n",
    "\n",
    "7. **Machine Learning Models:**\n",
    "   - **Strategy:** Use machine learning models to predict missing values based on other variables.\n",
    "   - **Explanation:** If the model performs well, missing values may be predictable based on the observed data, indicating a non-random pattern.\n",
    "\n",
    "8. **Imputation Methods:**\n",
    "   - **Strategy:** Apply different imputation methods and observe their impact on the analysis.\n",
    "   - **Explanation:** Compare results obtained using different imputation methods. If the results are sensitive to the imputation method, it may suggest a non-random pattern.\n",
    "\n",
    "It's important to note that these strategies are not mutually exclusive, and a combination of approaches may provide a more comprehensive understanding of missing data patterns. Additionally, consulting with domain experts and thoroughly exploring the data context can contribute to a more accurate assessment of missing data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bcdb8",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de4f00",
   "metadata": {},
   "source": [
    "Handling imbalanced datasets, especially in medical diagnosis projects where the occurrence of a condition of interest is often rare, is crucial for developing a robust and effective machine learning model. Here are some strategies to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "1. **Use Appropriate Evaluation Metrics:**\n",
    "   - Choose evaluation metrics that are suitable for imbalanced datasets. Common metrics include:\n",
    "     - **Precision:** The ratio of correctly predicted positive observations to the total predicted positives. Useful when the cost of false positives is high.\n",
    "     - **Recall (Sensitivity or True Positive Rate):** The ratio of correctly predicted positive observations to the total actual positives. Useful when the cost of false negatives is high.\n",
    "     - **F1-score:** The harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "2. **Confusion Matrix Analysis:**\n",
    "   - Examine the confusion matrix to get a detailed view of model performance. This includes true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "3. **Receiver Operating Characteristic (ROC) Curve:**\n",
    "   - Plot the ROC curve and calculate the area under the curve (AUC-ROC). This is especially useful for binary classification models, providing insights into the trade-off between sensitivity and specificity.\n",
    "\n",
    "4. **Precision-Recall (PR) Curve:**\n",
    "   - Plot the precision-recall curve, which focuses on the trade-off between precision and recall. This is particularly relevant when dealing with imbalanced datasets.\n",
    "\n",
    "5. **Stratified Sampling:**\n",
    "   - When splitting your dataset into training and testing sets, use stratified sampling to ensure that the class distribution is maintained in both sets.\n",
    "\n",
    "6. **Cross-Validation:**\n",
    "   - Perform cross-validation with stratified folds to ensure that each fold maintains the same class distribution as the original dataset.\n",
    "\n",
    "7. **Class Weights:**\n",
    "   - Adjust class weights in your machine learning model to give higher importance to the minority class. Many classifiers in scikit-learn provide a `class_weight` parameter that can be set to 'balanced' or manually adjusted based on the class distribution.\n",
    "\n",
    "8. **Ensemble Methods:**\n",
    "   - Use ensemble methods like Random Forest or Gradient Boosting, which can handle imbalanced datasets well. These methods often involve combining multiple weak learners to improve overall performance.\n",
    "\n",
    "9. **Resampling Techniques:**\n",
    "   - Explore resampling techniques such as oversampling the minority class, undersampling the majority class, or using more advanced methods like SMOTE (Synthetic Minority Over-sampling Technique) to balance the class distribution.\n",
    "\n",
    "\n",
    "It's essential to carefully choose and interpret evaluation metrics based on the specific goals and constraints of the medical diagnosis project. Additionally, a combination of the above strategies may be beneficial to achieve the best model performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c298e04",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d72d4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satisfied        200\n",
      "Not Satisfied    200\n",
      "Name: Customer_Satisfaction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample DataFrame with imbalanced data\n",
    "df = pd.DataFrame({'Customer_Satisfaction': ['Satisfied'] * 800 + ['Not Satisfied'] * 200,\n",
    "                   'Feature1': [1] * 800 + [0] * 200})\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['Customer_Satisfaction'] == 'Satisfied']\n",
    "minority_class = df[df['Customer_Satisfaction'] == 'Not Satisfied']\n",
    "\n",
    "# Down-sample majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "# Combine down-sampled majority class with minority class\n",
    "downsampled_data = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(downsampled_data['Customer_Satisfaction'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f22be",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f340eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    900\n",
      "1    900\n",
      "Name: Occurrence, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Random Over-sampling:\n",
    "\n",
    "# Description: Randomly replicate instances from the minority class to match the size of the majority class.\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample DataFrame with imbalanced data\n",
    "df = pd.DataFrame({'Occurrence': [1] * 100 + [0] * 900,\n",
    "                   'Feature1': [1] * 100 + [0] * 900})\n",
    "\n",
    "# Separate minority and majority classes\n",
    "minority_class = df[df['Occurrence'] == 1]\n",
    "majority_class = df[df['Occurrence'] == 0]\n",
    "\n",
    "# Up-sample minority class\n",
    "minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "\n",
    "# Combine up-sampled minority class with majority class\n",
    "upsampled_data = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "print(upsampled_data['Occurrence'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77b2ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    900\n",
      "0    900\n",
      "Name: Occurrence, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "# Description: Generate synthetic instances for the minority class using interpolation techniques.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Sample DataFrame with imbalanced data\n",
    "df = pd.DataFrame({'Occurrence': [1] * 100 + [0] * 900,\n",
    "                   'Feature1': [1] * 100 + [0] * 900})\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('Occurrence', axis=1)\n",
    "y = df['Occurrence']\n",
    "\n",
    "# Apply SMOTE over-sampling\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Combine features and target variable into a DataFrame\n",
    "upsampled_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Occurrence')], axis=1)\n",
    "\n",
    "print(upsampled_data['Occurrence'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc0c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
