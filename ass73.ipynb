{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faf6bbb",
   "metadata": {},
   "source": [
    "#### Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59a431",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data mining, machine learning, and statistics to identify patterns or instances that do not conform to expected behavior within a dataset. The purpose of anomaly detection is to discover unusual patterns, outliers, or deviations in data that might indicate a potential problem, anomaly, or interesting pattern that merits further investigation.\n",
    "\n",
    "Here's a breakdown of its purpose and significance:\n",
    "\n",
    "1. **Identification of Unusual Behavior**: Anomaly detection helps to pinpoint instances or patterns within a dataset that deviate significantly from the norm. These anomalies could represent potential errors, fraudulent activities, or interesting phenomena that require attention.\n",
    "\n",
    "2. **Early Detection of Issues**: By identifying anomalies early, organizations can address potential issues before they escalate. For example, in network security, detecting unusual traffic patterns might indicate a cyber attack in its early stages.\n",
    "\n",
    "3. **Improved Decision Making**: Anomaly detection can provide valuable insights for decision-making processes. By highlighting unusual patterns or outliers, businesses can make informed decisions to optimize processes, improve efficiency, or mitigate risks.\n",
    "\n",
    "4. **Fraud Detection**: Anomaly detection is extensively used in fraud detection systems across various industries such as banking, insurance, and e-commerce. Unusual patterns in financial transactions or customer behavior can indicate potential fraudulent activities.\n",
    "\n",
    "5. **Quality Control and Maintenance**: In manufacturing and other industries, anomaly detection can be used for quality control and predictive maintenance. Detecting anomalies in machinery sensor data, for example, can help prevent equipment failures and reduce downtime.\n",
    "\n",
    "6. **Healthcare Monitoring**: Anomaly detection is crucial in healthcare for identifying irregularities in patient data, such as vital signs or medical test results, which could indicate potential health issues or disease outbreaks.\n",
    "\n",
    "7. **Market Analysis**: In finance, anomaly detection is used for detecting unusual market behavior or financial transactions that might indicate market manipulation, insider trading, or systemic risks.\n",
    "\n",
    "8. **Natural Disaster Detection**: Anomaly detection can be applied to various environmental data, such as seismic activity, weather patterns, or ocean currents, to detect anomalies that might indicate natural disasters or environmental changes.\n",
    "\n",
    "Overall, anomaly detection plays a critical role in various domains by helping organizations identify unusual patterns, outliers, or deviations in data, enabling them to take proactive measures to address potential issues or capitalize on emerging opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf6e8c",
   "metadata": {},
   "source": [
    "#### Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c8950",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges, some of which include:\n",
    "\n",
    "1. **Labeling Anomalies**: In many cases, anomalies are rare events or patterns that are not explicitly labeled in the dataset. Therefore, determining what constitutes an anomaly and obtaining labeled data for training can be challenging.\n",
    "\n",
    "2. **Imbalanced Data**: Anomalies are often rare compared to normal instances, leading to imbalanced datasets. Traditional machine learning algorithms may struggle to effectively detect anomalies in such imbalanced data because they are biased towards the majority class.\n",
    "\n",
    "3. **Noise and Outliers**: Distinguishing between genuine anomalies and noise or outliers in the data can be difficult. Noisy data or outliers that are not indicative of true anomalies can lead to false positives and decrease the accuracy of anomaly detection models.\n",
    "\n",
    "4. **High Dimensionality**: Anomaly detection is particularly challenging in high-dimensional datasets where the number of features is large. In such cases, it becomes harder to identify meaningful patterns and anomalies due to the curse of dimensionality.\n",
    "\n",
    "5. **Concept Drift**: Anomalies may change over time due to evolving behaviors, trends, or external factors. Therefore, anomaly detection models need to be adaptive and robust to handle concept drift, where the underlying distribution of data changes over time.\n",
    "\n",
    "6. **Scalability**: For large-scale datasets, scalability becomes a significant challenge. Anomaly detection algorithms should be efficient and scalable to handle the volume of data generated by modern applications and systems.\n",
    "\n",
    "7. **Interpretability**: Understanding why a particular instance is flagged as an anomaly is crucial for decision-making and further investigation. However, many anomaly detection techniques lack interpretability, making it challenging to trust and validate the detected anomalies.\n",
    "\n",
    "8. **Contextual Information**: Anomalies often depend on the context in which they occur. Incorporating contextual information into anomaly detection models can improve their accuracy and relevance but adds complexity to the modeling process.\n",
    "\n",
    "9. **Adversarial Attacks**: In security-related applications, adversaries may intentionally manipulate data to evade detection by anomaly detection systems. Adversarial attacks pose a significant challenge and require robust anomaly detection techniques to mitigate.\n",
    "\n",
    "10. **Anomaly Types and Variability**: Anomalies can manifest in various forms, including point anomalies, contextual anomalies, and collective anomalies. Each type requires different detection techniques, and the variability of anomalies makes it challenging to design a one-size-fits-all solution.\n",
    "\n",
    "Addressing these challenges requires a combination of domain expertise, advanced algorithms, robust feature engineering, and continuous monitoring and adaptation of anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863249b8",
   "metadata": {},
   "source": [
    "#### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d3c5b",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two approaches used to identify anomalies in data, and they differ primarily in the availability of labeled data for training:\n",
    "\n",
    "1. **Unsupervised Anomaly Detection**:\n",
    "   - In unsupervised anomaly detection, the algorithm operates on a dataset without labeled anomalies.\n",
    "   - The goal is to identify patterns or instances that deviate significantly from the norm without prior knowledge of what constitutes an anomaly.\n",
    "   - Unsupervised methods typically rely on statistical techniques, clustering algorithms, or density estimation to detect anomalies based on their deviation from the majority of the data.\n",
    "   - Examples of unsupervised anomaly detection algorithms include k-means clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), isolation forest, and Gaussian mixture models.\n",
    "\n",
    "2. **Supervised Anomaly Detection**:\n",
    "   - In supervised anomaly detection, the algorithm requires labeled data where anomalies are explicitly marked.\n",
    "   - The algorithm learns to distinguish between normal instances and anomalies based on the labeled training data.\n",
    "   - Supervised methods typically involve training a classification model (e.g., decision trees, support vector machines, neural networks) on labeled data to predict whether new instances are normal or anomalous.\n",
    "   - The performance of supervised anomaly detection models heavily relies on the quality and representativeness of the labeled training data.\n",
    "   - Examples of supervised anomaly detection techniques include one-class SVM (Support Vector Machine), ensemble methods, and deep learning-based approaches.\n",
    "\n",
    "Key differences between unsupervised and supervised anomaly detection include:\n",
    "\n",
    "- **Availability of Labeled Data**: Unsupervised methods do not require labeled data for training, whereas supervised methods rely on labeled data to learn the distinction between normal and anomalous instances.\n",
    "  \n",
    "- **Scalability and Adaptability**: Unsupervised methods can be more scalable and adaptable since they do not require labeled data. They can detect anomalies in new data without the need for retraining. Supervised methods may require retraining when faced with new types of anomalies or changes in the data distribution.\n",
    "  \n",
    "- **Assumption of Anomaly Distribution**: Unsupervised methods make fewer assumptions about the distribution of anomalies since they do not rely on labeled data. Supervised methods may be biased towards the types of anomalies present in the training data.\n",
    "  \n",
    "- **Interpretability**: Supervised methods may provide better interpretability since they learn from labeled data, making it easier to understand why certain instances are classified as anomalies. Unsupervised methods may lack interpretability since they rely solely on data patterns without explicit labels.\n",
    "\n",
    "Both approaches have their advantages and limitations, and the choice between unsupervised and supervised anomaly detection depends on factors such as the availability of labeled data, the nature of anomalies, scalability requirements, and interpretability needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e31f4",
   "metadata": {},
   "source": [
    "#### Q4. What are the main categories of anomaly detection algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ed1df",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broadly categorized into several main categories based on their underlying techniques and methodologies. These categories include:\n",
    "\n",
    "1. **Statistical Methods**:\n",
    "   - Statistical methods assume that normal data instances follow a known statistical distribution (e.g., Gaussian distribution), and anomalies deviate significantly from this distribution.\n",
    "   - Techniques such as z-score, Grubbs' test, Dixon's Q test, and box plot analysis are commonly used for statistical anomaly detection.\n",
    "\n",
    "2. **Machine Learning-Based Methods**:\n",
    "   - Machine learning-based methods leverage algorithms to learn patterns from data and identify anomalies based on deviations from learned patterns.\n",
    "   - This category includes both supervised and unsupervised learning techniques.\n",
    "   - Supervised learning methods include algorithms like support vector machines (SVM), decision trees, and neural networks, where anomalies are identified based on labeled training data.\n",
    "   - Unsupervised learning methods include clustering algorithms (e.g., k-means, DBSCAN), density estimation techniques (e.g., Gaussian mixture models), and outlier detection algorithms (e.g., isolation forest, LOF).\n",
    "\n",
    "3. **Proximity-Based Methods**:\n",
    "   - Proximity-based methods measure the similarity or distance between data instances and use thresholds to identify instances that are significantly different from others.\n",
    "   - Techniques such as nearest neighbor-based methods (e.g., k-nearest neighbors) and distance-based outlier detection (e.g., Local Outlier Factor) fall into this category.\n",
    "\n",
    "4. **Information Theory-Based Methods**:\n",
    "   - Information theory-based methods quantify the amount of information needed to represent data instances and detect anomalies based on unusual information content.\n",
    "   - Entropy-based measures and information gain are examples of techniques used in information theory-based anomaly detection.\n",
    "\n",
    "5. **Clustering-Based Methods**:\n",
    "   - Clustering-based methods partition data into clusters based on similarity or density and identify anomalies as data points that do not belong to any cluster or belong to small, sparse clusters.\n",
    "   - Density-based clustering algorithms like DBSCAN and hierarchical clustering methods are commonly used for clustering-based anomaly detection.\n",
    "\n",
    "6. **Time Series-Based Methods**:\n",
    "   - Time series-based methods focus on detecting anomalies in sequential data over time.\n",
    "   - Techniques such as autoregressive models, moving averages, and seasonality detection are used to identify deviations from expected temporal patterns.\n",
    "\n",
    "7. **Deep Learning-Based Methods**:\n",
    "   - Deep learning-based methods utilize deep neural networks to automatically learn complex patterns and representations from data for anomaly detection.\n",
    "   - Autoencoders, recurrent neural networks (RNNs), and convolutional neural networks (CNNs) are commonly used architectures for deep learning-based anomaly detection.\n",
    "\n",
    "These categories are not mutually exclusive, and many anomaly detection algorithms combine techniques from multiple categories to improve detection performance and robustness. The choice of algorithm depends on factors such as the nature of the data, the type of anomalies, computational resources, and interpretability requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef05a8",
   "metadata": {},
   "source": [
    "#### Q5. What are the main assumptions made by distance-based anomaly detection methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475229e",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that anomalies are typically far from normal data points in the feature space. These methods make several key assumptions:\n",
    "\n",
    "1. **Euclidean Distance**: Distance-based methods often assume that the Euclidean distance metric adequately captures the similarity or dissimilarity between data points in the feature space. In other words, anomalies are expected to have significantly higher distances from normal instances.\n",
    "\n",
    "2. **Clustering Structure**: Some distance-based methods assume that normal data points tend to cluster together, forming dense regions in the feature space. Anomalies, on the other hand, are assumed to be isolated or located in sparse regions, far from normal clusters.\n",
    "\n",
    "3. **Nearest Neighbor Similarity**: Anomalies are expected to have few or no similar neighbors in the feature space. Distance-based methods often identify anomalies as data points that are distant from their nearest neighbors or have low similarity scores compared to their neighbors.\n",
    "\n",
    "4. **Thresholding**: Distance-based anomaly detection methods typically use a threshold to distinguish between normal and anomalous data points. The assumption is that data points beyond a certain distance threshold from the majority of the data are considered anomalies.\n",
    "\n",
    "5. **Feature Independence**: Some distance-based methods assume that features are independent of each other or have equal importance in determining distances. However, this assumption may not hold in all cases, especially when dealing with high-dimensional or correlated data.\n",
    "\n",
    "6. **Data Distribution**: Distance-based methods may assume a particular distribution of normal data points in the feature space, such as a Gaussian distribution. Anomalies are then identified as data points that deviate significantly from this assumed distribution.\n",
    "\n",
    "7. **Homogeneity**: These methods often assume that the underlying data distribution is homogeneous, meaning that anomalies are uniformly distributed across the feature space. However, in practice, anomalies may exhibit localized patterns or clusters.\n",
    "\n",
    "It's essential to note that while distance-based methods make these assumptions, the effectiveness of anomaly detection depends on how well these assumptions hold true for the specific dataset and type of anomalies present. Additionally, distance-based methods may not perform well in high-dimensional spaces or when dealing with complex data distributions. As such, it's crucial to carefully assess the suitability of distance-based methods and consider alternative approaches when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e987af",
   "metadata": {},
   "source": [
    "#### Q6. How does the LOF algorithm compute anomaly scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24669293",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores for data points based on their deviation from the local density of their neighbors. Here's how the LOF algorithm works:\n",
    "\n",
    "1. **Local Density Calculation**:\n",
    "   - For each data point \\( p \\), the algorithm identifies its \\( k \\) nearest neighbors in the feature space.\n",
    "   - The distance metric used for determining neighbors can be Euclidean distance, Manhattan distance, or any other appropriate distance measure.\n",
    "   - The local density of point \\( p \\) is estimated based on the distance to its \\( k \\) nearest neighbors. Generally, a higher density indicates that the point resides in a dense region of the feature space.\n",
    "\n",
    "2. **Local Reachability Distance Calculation**:\n",
    "   - For each neighbor \\( q \\) of point \\( p \\), the local reachability distance (LRD) of \\( q \\) with respect to \\( p \\) is computed. The LRD of \\( q \\) measures how reachable \\( q \\) is from \\( p \\) based on the density of \\( p \\) and the distance between \\( p \\) and \\( q \\).\n",
    "   - The LRD of \\( q \\) is calculated as the inverse of the average reachability distance of \\( q \\) from its \\( k \\) nearest neighbors. It quantifies how close the neighbors of \\( p \\) are to each other relative to their distances from \\( p \\).\n",
    "\n",
    "3. **Local Outlier Factor Calculation**:\n",
    "   - The local outlier factor (LOF) of point \\( p \\) is computed based on its LRD and the LRDs of its neighbors.\n",
    "   - The LOF of \\( p \\) is defined as the average ratio of the LRD of \\( p \\) to the LRDs of its neighbors. A higher LOF indicates that point \\( p \\) is less similar to its neighbors in terms of density, suggesting that it may be an outlier.\n",
    "\n",
    "4. **Anomaly Score Assignment**:\n",
    "   - The anomaly score of each data point is determined based on its LOF value. Points with higher LOF values are considered more anomalous, while points with lower LOF values are considered more similar to their local neighborhoods and hence less anomalous.\n",
    "\n",
    "In summary, the LOF algorithm assigns anomaly scores to data points based on their deviation from the local density of their neighbors. Points with low local density compared to their neighbors are assigned higher anomaly scores, indicating that they are potential outliers or anomalies in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c79e0",
   "metadata": {},
   "source": [
    "#### Q7. What are the key parameters of the Isolation Forest algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d207e",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised anomaly detection algorithm based on the concept of isolating anomalies in a dataset using binary trees. It works by randomly selecting features and splitting data points until anomalies are isolated into smaller partitions. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1. **n_estimators**: This parameter determines the number of isolation trees to be built. Increasing the number of trees generally improves the performance of the Isolation Forest but also increases computation time. Typically, a larger number of trees leads to better anomaly detection accuracy.\n",
    "\n",
    "2. **max_samples**: It specifies the number of samples to be drawn from the dataset to build each isolation tree. A smaller value leads to shorter trees and potentially better detection of outliers. However, setting it too low might result in trees that are not representative of the dataset.\n",
    "\n",
    "3. **max_features**: This parameter controls the number of features to be considered when splitting a node in the isolation tree. A smaller value can speed up the algorithm and reduce memory usage. It also reduces the correlation between individual trees, potentially improving the robustness of the algorithm.\n",
    "\n",
    "4. **contamination**: This parameter specifies the expected proportion of anomalies in the dataset. It helps to adjust the threshold for classifying data points as anomalies. Setting it too high or too low can affect the accuracy of anomaly detection.\n",
    "\n",
    "5. **bootstrap**: When set to True, this parameter enables bootstrapping of samples when building each isolation tree. Bootstrapping helps to introduce randomness and diversity into the trees, reducing the risk of overfitting.\n",
    "\n",
    "6. **random_state**: It sets the random seed used by the random number generator. Setting a specific random state ensures reproducibility of results across multiple runs.\n",
    "\n",
    "7. **behaviour**: This parameter determines the behavior of the algorithm when handling outliers. Setting it to 'new' allows the algorithm to handle unseen data points better, while setting it to 'old' makes it behave like the original Isolation Forest implementation.\n",
    "\n",
    "These parameters provide control over the behavior and performance of the Isolation Forest algorithm, allowing users to customize it according to the characteristics of their dataset and the specific requirements of their anomaly detection task. Adjusting these parameters appropriately can lead to improved anomaly detection performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1944a0",
   "metadata": {},
   "source": [
    "#### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "#### using KNN with K=10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ac053",
   "metadata": {},
   "source": [
    "To compute the anomaly score of a data point using the k-nearest neighbors (KNN) algorithm with \\(K=10\\), we first need to understand how the anomaly score is typically calculated. In KNN-based anomaly detection, the anomaly score of a data point is often defined as the average distance to its \\(K\\) nearest neighbors. The intuition is that outliers or anomalies will have larger average distances to their neighbors compared to normal data points.\n",
    "\n",
    "In this scenario, if a data point has only 2 neighbors of the same class within a radius of 0.5 and \\(K=10\\), it means that the remaining \\(K-2 = 8\\) nearest neighbors are either of a different class or are further away than the radius of 0.5.\n",
    "\n",
    "Therefore, the anomaly score for this data point can be calculated as the average distance to its 10 nearest neighbors, where 2 neighbors are within the radius of 0.5 and the remaining 8 neighbors are beyond the radius.\n",
    "\n",
    "Let's assume the distances to the 10 nearest neighbors are as follows:\n",
    "\n",
    "- 2 neighbors within the radius of 0.5: \\(d_1 = 0.3\\) and \\(d_2 = 0.4\\).\n",
    "- 8 neighbors beyond the radius: \\(d_3, d_4, ..., d_{10}\\).\n",
    "\n",
    "The anomaly score (\\(AS\\)) can be calculated as:\n",
    "\n",
    "\\[ AS = \\frac{1}{K} \\sum_{i=1}^{K} d_i \\]\n",
    "\n",
    "\\[ AS = \\frac{1}{10} \\left( d_1 + d_2 + \\sum_{i=3}^{10} d_i \\right) \\]\n",
    "\n",
    "\\[ AS = \\frac{1}{10} \\left( 0.3 + 0.4 + \\sum_{i=3}^{10} d_i \\right) \\]\n",
    "\n",
    "Since we don't have specific values for \\(d_3, d_4, ..., d_{10}\\), we cannot compute the exact anomaly score. However, we can say that the anomaly score will be influenced by the distances to the remaining 8 neighbors, which are beyond the radius of 0.5. Typically, larger distances would contribute to a higher anomaly score.\n",
    "\n",
    "So, in summary, the anomaly score of the data point using KNN with \\(K=10\\) and 2 neighbors of the same class within a radius of 0.5 will be influenced by the distances to the remaining 8 neighbors, but the exact value cannot be determined without knowing these distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4df835",
   "metadata": {},
   "source": [
    "#### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "#### anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "#### length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27d2f8",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is typically computed based on its average path length (APL) compared to the average path length of the trees in the forest. The APL represents the average number of edges traversed to isolate a data point in all trees of the forest.\n",
    "\n",
    "Given that we have a dataset with 3000 data points and a forest of 100 trees, and a data point with an average path length of 5.0, we can compute the anomaly score using the following formula:\n",
    "\n",
    "\\[ Anomaly \\, Score = 2^{-\\frac{APL}{c(n)}} \\]\n",
    "\n",
    "Where:\n",
    "- \\( APL \\) is the average path length of the data point (in this case, 5.0).\n",
    "- \\( n \\) is the number of data points in the dataset (3000 in this case).\n",
    "- \\( c(n) \\) is the average path length of unsuccessful searches in a binary tree of \\( n \\) data points, which can be approximated by \\( 2 \\cdot \\log_2(n-1) - \\frac{2 \\cdot (n-1)}{n} \\).\n",
    "\n",
    "Substituting the values into the formula:\n",
    "\n",
    "\\[ Anomaly \\, Score = 2^{-\\frac{5.0}{c(3000)}} \\]\n",
    "\n",
    "First, let's calculate \\( c(3000) \\):\n",
    "\n",
    "\\[ c(3000) = 2 \\cdot \\log_2(3000-1) - \\frac{2 \\cdot (3000-1)}{3000} \\]\n",
    "\n",
    "\\[ c(3000) \\approx 2 \\cdot \\log_2(2999) - \\frac{2 \\cdot 2999}{3000} \\]\n",
    "\n",
    "\\[ c(3000) \\approx 2 \\cdot 11.550746785383243 - 1.9993333333333334 \\]\n",
    "\n",
    "\\[ c(3000) \\approx 21.101493570766486 - 1.9993333333333334 \\]\n",
    "\n",
    "\\[ c(3000) \\approx 19.102160237433153 \\]\n",
    "\n",
    "Now, substitute \\( c(3000) \\) into the anomaly score formula:\n",
    "\n",
    "\\[ Anomaly \\, Score = 2^{-\\frac{5.0}{19.102160237433153}} \\]\n",
    "\n",
    "\\[ Anomaly \\, Score \\approx 2^{-0.2617957987332755} \\]\n",
    "\n",
    "\\[ Anomaly \\, Score \\approx 0.7405352080 \\]\n",
    "\n",
    "So, the anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees in the forest is approximately 0.7405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ed425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
