{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d35555e",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0d208",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and effective supervised machine learning algorithm used for classification and regression tasks. It operates on the principle that similar data points tend to belong to the same class or have similar numeric values. \n",
    "\n",
    "In the KNN algorithm:\n",
    "\n",
    "1. **Training Phase**: The algorithm stores all available labeled data points.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - For a given input data point, the algorithm calculates the distances between that point and all other data points in the training set using a distance metric such as Euclidean distance.\n",
    "   - It then selects the K-nearest neighbors (data points with the smallest distances) to the input data point.\n",
    "   - For classification tasks, the algorithm assigns the most common class label among the K-nearest neighbors to the input data point. In regression tasks, it calculates the average (or weighted average) of the target values of the K-nearest neighbors.\n",
    "   - The value of K is a hyperparameter that needs to be specified beforehand. It greatly influences the performance of the algorithm.\n",
    "\n",
    "KNN is a non-parametric algorithm, meaning it does not make any assumptions about the underlying data distribution. It's also considered a lazy learner because it doesn't explicitly build a model during training, but rather memorizes the training dataset and makes predictions based on similarity measures during inference. KNN can be computationally expensive, especially when dealing with large datasets or high-dimensional data, as it requires calculating distances for each query instance against all training samples. However, it's easy to implement and understand, making it a popular choice for many classification and regression tasks, especially for small to moderate-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb115623",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085a888",
   "metadata": {},
   "source": [
    "Choosing the appropriate value of K in K-Nearest Neighbors (KNN) is crucial for the performance of the algorithm. The value of K significantly affects the model's bias-variance tradeoff and its ability to generalize to unseen data. Here are some common methods to choose the value of K:\n",
    "\n",
    "1. **Domain Knowledge**: Prior knowledge about the problem domain and understanding of the data can provide insights into choosing an appropriate value of K. For example, if the problem involves distinguishing between classes that are inherently close or have a lot of overlap, a smaller value of K might be more suitable.\n",
    "\n",
    "2. **Grid Search and Cross-Validation**: Grid search, combined with cross-validation, is a systematic approach to selecting the best hyperparameters, including K. In this method, you specify a range of possible values for K, and then perform cross-validation (e.g., k-fold cross-validation) with each value of K. The value of K that yields the best performance metric (e.g., accuracy, F1-score) on the validation set is selected as the optimal value.\n",
    "\n",
    "3. **Odd Values**: When dealing with binary classification problems, it's common to choose odd values of K to avoid ties. For example, if there are two classes and K is even, it's possible to have a tie in the voting process, which might lead to ambiguous predictions. Using odd values of K helps to break ties.\n",
    "\n",
    "4. **Rule of Thumb**: As a general rule, the value of K can be set to the square root of the total number of data points in the training set. However, this is just a heuristic and might not always result in the best performance.\n",
    "\n",
    "5. **Experimentation**: Sometimes, experimenting with different values of K and evaluating the model's performance on a validation set or through cross-validation can provide insights into which value of K works best for the specific dataset and problem at hand.\n",
    "\n",
    "It's important to note that the choice of K is problem-dependent, and there is no one-size-fits-all solution. Therefore, it's recommended to try multiple values of K and evaluate their performance using appropriate evaluation metrics before finalizing the value of K for a given dataset and task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4d25b",
   "metadata": {},
   "source": [
    "\n",
    "### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d73d7",
   "metadata": {},
   "source": [
    "The main difference between the K-Nearest Neighbors (KNN) classifier and the KNN regressor lies in their respective tasks:\n",
    "\n",
    "1. **KNN Classifier**:\n",
    "   - KNN classifier is used for classification tasks, where the goal is to classify data points into predefined classes or categories.\n",
    "   - It predicts the class label of a new data point based on the majority class among its K-nearest neighbors.\n",
    "   - The output of the KNN classifier is a class label or a categorical value.\n",
    "   - The decision boundary in KNN classifier is nonlinear and is determined by the distribution of the training data points.\n",
    "\n",
    "2. **KNN Regressor**:\n",
    "   - KNN regressor is used for regression tasks, where the goal is to predict continuous numeric values.\n",
    "   - It predicts the numeric value of a new data point based on the average (or weighted average) of the target values of its K-nearest neighbors.\n",
    "   - The output of the KNN regressor is a numeric value.\n",
    "   - In KNN regression, the prediction is based on the average of the target values of nearby data points, resulting in a smooth and continuous prediction surface.\n",
    "\n",
    "In summary, while both KNN classifier and KNN regressor use the same underlying principle of finding the K-nearest neighbors to make predictions, they differ in terms of the nature of the prediction task (classification vs. regression) and the type of output they produce (class labels vs. numeric values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66b04a",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3289b72",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) model can be evaluated using various metrics depending on the nature of the task, whether it's a classification or regression problem. Here are some common evaluation metrics for both classification and regression tasks:\n",
    "\n",
    "**For Classification Tasks:**\n",
    "\n",
    "1. **Accuracy**: Accuracy measures the proportion of correctly classified instances out of the total instances. It's a simple and intuitive metric but can be misleading in the presence of class imbalance.\n",
    "\n",
    "2. **Precision, Recall, and F1-score**: These metrics are particularly useful when dealing with imbalanced datasets. Precision measures the proportion of true positive predictions among all positive predictions, recall measures the proportion of true positive predictions among all actual positives, and F1-score is the harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "3. **Confusion Matrix**: A confusion matrix provides a detailed breakdown of the model's predictions, showing the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix, various performance metrics such as precision, recall, and accuracy can be derived.\n",
    "\n",
    "4. **ROC Curve and AUC-ROC**: Receiver Operating Characteristic (ROC) curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Area Under the ROC Curve (AUC-ROC) summarizes the ROC curve into a single scalar value, indicating the model's ability to distinguish between classes.\n",
    "\n",
    "**For Regression Tasks:**\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: MAE measures the average absolute difference between the predicted values and the actual values. It provides a straightforward interpretation of the model's performance in terms of the average magnitude of errors.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: MSE measures the average squared difference between the predicted values and the actual values. Squaring the errors penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**: RMSE is the square root of the MSE, providing an interpretable measure of the average magnitude of errors in the same units as the target variable.\n",
    "\n",
    "4. **R-squared (RÂ²)**: R-squared measures the proportion of variance explained by the model. It ranges from 0 to 1, where higher values indicate a better fit of the model to the data.\n",
    "\n",
    "In summary, the choice of performance metrics for evaluating KNN models depends on the specific task and the nature of the data. It's often recommended to use multiple metrics to get a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a512bc",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16ada4",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the phenomenon where the performance of certain algorithms, including K-Nearest Neighbors (KNN), deteriorates significantly as the dimensionality of the feature space increases. In the context of KNN, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "1. **Increased Computational Complexity**: As the number of dimensions (features) in the dataset increases, the computational complexity of KNN grows exponentially. This is because computing distances between data points becomes more computationally expensive in higher-dimensional spaces. Consequently, the time required to search for nearest neighbors increases substantially.\n",
    "\n",
    "2. **Data Sparsity**: In high-dimensional spaces, data points become sparse, meaning that the data are spread out over a larger volume. As a result, the notion of \"closeness\" becomes less meaningful, making it difficult to identify meaningful neighbors. This can lead to increased noise in the nearest neighbor search, resulting in poorer predictive performance.\n",
    "\n",
    "3. **Increased Data Requirements**: With higher dimensions, the amount of data required to effectively cover the feature space grows exponentially. This means that to maintain the same level of data density, an exponentially larger dataset is needed as the dimensionality increases. Gathering and storing such large datasets may not be feasible in many real-world scenarios.\n",
    "\n",
    "4. **Diminished Discriminative Power**: High-dimensional spaces often exhibit increased variability and complexity, making it more challenging to discriminate between different classes or groups of data points. This can lead to decreased predictive accuracy and generalization performance of the KNN algorithm.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other machine learning algorithms, practitioners often employ techniques such as dimensionality reduction (e.g., PCA, t-SNE), feature selection, or feature engineering to reduce the number of dimensions in the dataset while retaining relevant information. Additionally, using distance metrics that are less sensitive to high-dimensional spaces or employing specialized algorithms designed to handle high-dimensional data can also help alleviate the effects of the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3aa70c",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de80c0",
   "metadata": {},
   "source": [
    "Handling missing values in the context of K-Nearest Neighbors (KNN) involves imputing or estimating the missing values before applying the algorithm. Here are several common approaches:\n",
    "\n",
    "1. **Simple Imputation**:\n",
    "   - Fill missing values with a constant: For numerical features, you can replace missing values with a predefined constant (e.g., mean, median, or mode of the feature).\n",
    "   - Mode imputation for categorical features: For categorical features, you can replace missing values with the mode (most frequent category) of the feature.\n",
    "\n",
    "2. **KNN Imputation**:\n",
    "   - Use KNN to impute missing values: In this approach, you treat each missing value as a prediction problem. You use KNN to find the K-nearest neighbors of the data point with the missing value and then impute the missing value with the average (or weighted average) of the corresponding feature values of its neighbors.\n",
    "\n",
    "3. **Multiple Imputation**:\n",
    "   - Generate multiple imputations: Rather than imputing missing values with a single value, you can generate multiple imputations to capture the uncertainty associated with the missing data. This involves creating several imputed datasets, running KNN or other imputation methods on each dataset, and then combining the results using appropriate statistical techniques.\n",
    "\n",
    "4. **Model-Based Imputation**:\n",
    "   - Use other machine learning models to predict missing values: Instead of KNN, you can use other predictive models such as linear regression, decision trees, or random forests to predict missing values based on the available data.\n",
    "\n",
    "5. **Missing Indicator**:\n",
    "   - Create a binary indicator variable: Instead of imputing missing values directly, you can create an additional binary variable indicating whether a value is missing or not. This allows the KNN algorithm to utilize the information about missingness during the prediction process.\n",
    "\n",
    "6. **Domain-Specific Knowledge**:\n",
    "   - Utilize domain knowledge: Sometimes, domain-specific knowledge can provide insights into the missing data mechanism or appropriate imputation strategies. For example, certain missing values might be systematically related to other variables in the dataset, allowing for more informed imputation methods.\n",
    "\n",
    "It's essential to carefully consider the characteristics of the dataset, the nature of the missing data, and the potential impact of different imputation strategies on the performance of the KNN algorithm. Additionally, evaluating the performance of different imputation methods using appropriate validation techniques is crucial to ensure the reliability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302ad8a",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "### which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7f78b",
   "metadata": {},
   "source": [
    "Comparing and contrasting the performance of KNN classifier and regressor depends on the specific characteristics of the dataset and the nature of the prediction task. Here's a comparison between the two:\n",
    "\n",
    "**KNN Classifier:**\n",
    "\n",
    "- **Task**: Classification tasks involve predicting the class label of a data point.\n",
    "- **Output**: The output of a KNN classifier is a categorical label.\n",
    "- **Evaluation Metrics**: Common evaluation metrics include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "- **Decision Boundary**: KNN classifier's decision boundary is nonlinear and determined by the distribution of the training data points.\n",
    "- **Use Cases**: KNN classifier is suitable for problems where the target variable consists of discrete and unordered categories. It's commonly used in text categorization, image classification, and medical diagnosis.\n",
    "\n",
    "**KNN Regressor:**\n",
    "\n",
    "- **Task**: Regression tasks involve predicting a continuous numeric value.\n",
    "- **Output**: The output of a KNN regressor is a numeric value.\n",
    "- **Evaluation Metrics**: Common evaluation metrics include mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R-squared (RÂ²).\n",
    "- **Prediction Surface**: KNN regressor's prediction surface is smooth and continuous, as it predicts the average of the target values of neighboring data points.\n",
    "- **Use Cases**: KNN regressor is suitable for problems where the target variable is continuous and the relationships between features and target are linear or nonlinear. It's commonly used in housing price prediction, demand forecasting, and time series analysis.\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "- **Performance**: The performance of KNN classifier and regressor depends on factors such as the dataset's characteristics, the choice of distance metric, the value of K, and the presence of noise or outliers.\n",
    "- **Complexity**: KNN classifier is generally simpler to implement and understand compared to KNN regressor, as it deals with categorical labels rather than continuous values.\n",
    "- **Interpretability**: KNN classifier's predictions are directly interpretable as class labels, while KNN regressor's predictions may require additional interpretation, especially when dealing with complex relationships between features and target variables.\n",
    "\n",
    "**Selection Criteria**:\n",
    "\n",
    "- Choose KNN classifier for classification tasks with discrete and unordered categories.\n",
    "- Choose KNN regressor for regression tasks with continuous target variables and linear or nonlinear relationships between features and targets.\n",
    "- Consider the dataset's characteristics, the problem domain, and the interpretability of the predictions when selecting between KNN classifier and regressor.\n",
    "\n",
    "Ultimately, the selection between KNN classifier and regressor depends on the specific requirements and characteristics of the problem at hand, as well as the desired interpretability of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d4201",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "### and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128e41b",
   "metadata": {},
   "source": [
    "Here are the strengths and weaknesses of the K-Nearest Neighbors (KNN) algorithm for both classification and regression tasks, along with potential ways to address them:\n",
    "\n",
    "**Strengths of KNN:**\n",
    "\n",
    "1. **Simple to Implement**: KNN is easy to understand and implement, making it suitable for beginners and quick prototyping.\n",
    "\n",
    "2. **Non-parametric**: KNN makes no assumptions about the underlying data distribution, making it suitable for a wide range of data types and distributions.\n",
    "\n",
    "3. **Adaptability to Complex Decision Boundaries**: KNN can capture complex decision boundaries, especially when the dataset is nonlinear or has intricate class relationships.\n",
    "\n",
    "4. **No Training Phase**: KNN is a lazy learner, meaning it does not explicitly build a model during the training phase. This allows for efficient updating of the model with new data.\n",
    "\n",
    "5. **Robust to Outliers**: KNN is robust to outliers since it relies on the majority vote of the nearest neighbors.\n",
    "\n",
    "**Weaknesses of KNN:**\n",
    "\n",
    "1. **Computationally Expensive**: KNN's prediction time increases linearly with the size of the training dataset and the number of dimensions. For large datasets or high-dimensional data, the computation can become prohibitively expensive.\n",
    "\n",
    "2. **Sensitivity to Distance Metric and K Value**: The performance of KNN heavily depends on the choice of distance metric and the value of K. Poor choices can lead to suboptimal results.\n",
    "\n",
    "3. **Requires Sufficient Data**: KNN requires a sufficient amount of training data to effectively capture the underlying patterns in the data. Sparse datasets or datasets with high dimensionality may lead to unreliable predictions.\n",
    "\n",
    "4. **Not Suitable for Imbalanced Datasets**: In classification tasks, KNN may be biased towards the majority class in imbalanced datasets, leading to poor performance for minority classes.\n",
    "\n",
    "**Addressing the Weaknesses:**\n",
    "\n",
    "1. **Dimensionality Reduction**: To address computational complexity, consider reducing the dimensionality of the dataset using techniques such as principal component analysis (PCA) or feature selection before applying KNN.\n",
    "\n",
    "2. **Optimizing Hyperparameters**: Experiment with different distance metrics (e.g., Euclidean, Manhattan, Minkowski) and values of K through cross-validation to find the optimal combination for your dataset.\n",
    "\n",
    "3. **Data Preprocessing**: Handle missing values, normalize or standardize features, and address outliers before applying KNN to improve its performance.\n",
    "\n",
    "4. **Ensemble Methods**: Combine multiple KNN models or use ensemble techniques such as bagging or boosting to improve prediction accuracy and robustness.\n",
    "\n",
    "5. **Resampling Techniques**: Employ resampling techniques such as oversampling or undersampling to address class imbalance issues in classification tasks.\n",
    "\n",
    "6. **Use Approximate Nearest Neighbors (ANN)**: For large datasets, consider using ANN libraries like Annoy or FAISS to speed up the nearest neighbor search process.\n",
    "\n",
    "7. **Consider Alternative Algorithms**: If KNN's weaknesses persist even after optimization, consider alternative algorithms such as decision trees, random forests, or support vector machines (SVMs) that may offer better performance for your specific problem.\n",
    "\n",
    "By understanding and addressing these strengths and weaknesses, you can effectively leverage the KNN algorithm for classification and regression tasks in various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21861e7",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5aca38",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in K-Nearest Neighbors (KNN) algorithm for measuring the distance between data points. Here are the main differences between the two:\n",
    "\n",
    "1. **Euclidean Distance**:\n",
    "   - Euclidean distance is the straight-line distance between two points in Euclidean space.\n",
    "   - It is calculated as the square root of the sum of the squares of the differences between corresponding coordinates.\n",
    "   - In two dimensions, the Euclidean distance between two points (x1, y1) and (x2, y2) is given by:\n",
    "     ```\n",
    "     distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "     ```\n",
    "   - Euclidean distance is sensitive to the magnitude and scale of the features.\n",
    "\n",
    "2. **Manhattan Distance**:\n",
    "   - Manhattan distance, also known as city block distance or L1 distance, measures the distance between two points by summing the absolute differences of their coordinates.\n",
    "   - It is calculated as the sum of the absolute differences between corresponding coordinates.\n",
    "   - In two dimensions, the Manhattan distance between two points (x1, y1) and (x2, y2) is given by:\n",
    "     ```\n",
    "     distance = |x2 - x1| + |y2 - y1|\n",
    "     ```\n",
    "   - Manhattan distance is less sensitive to outliers and feature scale compared to Euclidean distance.\n",
    "\n",
    "**Differences:**\n",
    "\n",
    "1. **Shape of Distance**: Euclidean distance measures the shortest path (straight-line distance) between two points, whereas Manhattan distance measures the distance along the grid-like paths formed by the axes (horizontal and vertical).\n",
    "\n",
    "2. **Sensitivity to Features**: Euclidean distance considers both the magnitude and direction of differences between feature values, whereas Manhattan distance only considers the magnitude. Therefore, Euclidean distance is more sensitive to the scale and magnitude of features compared to Manhattan distance.\n",
    "\n",
    "3. **Axis Independence**: Manhattan distance is sometimes preferred when dealing with high-dimensional data or when the dimensions are not equally important because it is independent of coordinate axis orientation. In contrast, Euclidean distance considers the diagonal path and may be more influenced by correlations between features.\n",
    "\n",
    "In summary, while both Euclidean and Manhattan distances are valid distance metrics used in KNN, the choice between them depends on the characteristics of the dataset, the underlying problem, and the desired behavior of the algorithm with respect to feature scale and orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd165516",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83aa6ba",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in K-Nearest Neighbors (KNN) algorithm, primarily due to the way KNN calculates distances between data points. Here's how feature scaling affects KNN:\n",
    "\n",
    "1. **Distance Calculation**: KNN relies on calculating distances between data points to determine the nearest neighbors. Common distance metrics include Euclidean distance, Manhattan distance, or Minkowski distance. These distance metrics are sensitive to the scale and magnitude of features.\n",
    "\n",
    "2. **Impact of Feature Magnitude**: Features with larger magnitudes or scales can dominate the distance calculation process, overshadowing the contributions of features with smaller magnitudes. This can result in inaccurate distance measurements and biased nearest neighbor selection.\n",
    "\n",
    "3. **Equal Contribution of Features**: Feature scaling ensures that all features contribute equally to the distance calculation process. By scaling features to a common scale or range, each feature's impact on the distance metric becomes comparable, preventing any single feature from dominating the distance calculation.\n",
    "\n",
    "4. **Improving Model Performance**: Properly scaled features can lead to more accurate and reliable predictions in KNN. It helps prevent instances where the algorithm is biased towards certain features simply because of their larger scale, leading to more balanced and representative nearest neighbor selection.\n",
    "\n",
    "5. **Handling Numerical Instability**: Feature scaling can also help mitigate numerical instability issues, especially when dealing with algorithms that involve iterative optimization procedures, by ensuring that the numerical computations remain well-conditioned and stable.\n",
    "\n",
    "Common techniques for feature scaling in KNN include:\n",
    "\n",
    "- **Min-Max Scaling**: Scaling features to a specified range, typically between 0 and 1. This involves subtracting the minimum value and dividing by the range (maximum value minus minimum value) of each feature.\n",
    "\n",
    "- **Standardization (Z-score normalization)**: Scaling features to have a mean of 0 and a standard deviation of 1. This involves subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "- **Robust Scaling**: Scaling features based on robust statistics such as median and interquartile range, which are less affected by outliers compared to mean and standard deviation.\n",
    "\n",
    "By appropriately scaling the features before applying KNN, you can ensure that the algorithm operates effectively and produces reliable results, regardless of the scale or magnitude of the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
