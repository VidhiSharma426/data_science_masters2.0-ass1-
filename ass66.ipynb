{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afae6ad8",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd26a6c",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It belongs to the class of ensemble learning methods, specifically boosting algorithms. Gradient Boosting Regression builds a strong predictive model by combining multiple weak learners, typically decision trees, in a sequential manner.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization**: Gradient Boosting Regression starts with an initial model, often a simple one like the mean of the target variable. This initial model serves as the base prediction.\n",
    "\n",
    "2. **Building the Ensemble**: In each iteration, a weak learner (usually a decision tree) is added to the ensemble to correct the errors made by the existing model. The new weak learner is trained on the residuals (the differences between the actual target values and the predictions of the current ensemble).\n",
    "\n",
    "3. **Gradient Descent**: Instead of updating the model directly, Gradient Boosting Regression optimizes the model parameters by using gradient descent to minimize a loss function. The loss function quantifies the difference between the actual target values and the predictions of the ensemble.\n",
    "\n",
    "4. **Sequential Learning**: Each weak learner is trained sequentially, and the process continues for a specified number of iterations or until a stopping criterion is met. At each iteration, the new weak learner is trained to predict the residuals of the previous ensemble.\n",
    "\n",
    "5. **Combining Predictions**: Finally, the predictions of all weak learners are combined to produce the final ensemble prediction. The predictions are typically weighted based on the performance of each weak learner during training.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex nonlinear relationships in the data and its robustness against overfitting. However, it may require careful tuning of hyperparameters and can be computationally expensive, especially for large datasets. Overall, Gradient Boosting Regression is a powerful technique for regression tasks, often achieving state-of-the-art performance when properly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a214c",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "### simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "### performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d762339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 112732.75714231138\n",
      "R-squared: -79.85688419787752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp/ipykernel_20216/3466308091.py:32: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  predictions = np.sum(tree.predict(X) for tree in self.models)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize residuals with the target values\n",
    "        self.residuals = y.copy()\n",
    "\n",
    "        # Iterate over the number of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, self.residuals)\n",
    "\n",
    "            # Make predictions with the tree\n",
    "            predictions = tree.predict(X)\n",
    "\n",
    "            # Update residuals using the predictions\n",
    "            self.residuals -= self.learning_rate * predictions\n",
    "\n",
    "            # Save the tree to the list of models\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions with each tree and sum them up\n",
    "        predictions = np.sum(tree.predict(X) for tree in self.models)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Make predictions\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Calculate mean squared error\n",
    "        mse = np.mean((y_pred - y) ** 2)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_residual = np.sum((y - y_pred) ** 2)\n",
    "        r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "        return mse, r_squared\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "mse, r_squared = gb_regressor.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f8328",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "### optimise the performance of the model. Use grid search or random search to find the best\n",
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2031e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "mse, r_squared = best_model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error (Best Model):\", mse)\n",
    "print(\"R-squared (Best Model):\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae87f06",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e8467",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing on a given problem. These weak learners are typically decision trees with limited depth, often referred to as \"stumps\" or \"shallow trees.\" \n",
    "\n",
    "Weak learners are used iteratively in Gradient Boosting to improve the overall predictive performance of the model. In each iteration, a weak learner is fit to the residuals (the differences between the predicted values and the actual target values) of the previous iterations, with the goal of reducing these residuals. \n",
    "\n",
    "The key idea behind using weak learners in Gradient Boosting is that by sequentially adding multiple weak learners, each one focusing on the mistakes of the previous ones, the model gradually learns to correct its errors and improve its predictive accuracy. Despite their simplicity, these weak learners collectively form a strong ensemble model when combined together in the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf52f6",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6568bd4",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to sequentially train a series of weak learners (often decision trees) and combine their predictions in a way that minimizes the overall prediction error. This is achieved by iteratively fitting new weak learners to the residuals of the previous ones, with each new learner focusing on the mistakes made by the ensemble of learners learned so far.\n",
    "\n",
    "Here's a more detailed breakdown of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. **Sequential Learning**: Gradient Boosting builds an ensemble of weak learners sequentially, where each new learner learns to correct the mistakes of the previous ones. This sequential learning process allows the model to gradually improve its predictive performance over multiple iterations.\n",
    "\n",
    "2. **Gradient Descent**: At each iteration, Gradient Boosting fits a weak learner to the residuals of the current ensemble model. The residuals represent the errors made by the current model on the training data. Instead of directly fitting the weak learner to the target values, Gradient Boosting fits it to the negative gradient of the loss function with respect to the predictions of the current model. This negative gradient points in the direction of the steepest decrease in the loss function, allowing the new learner to focus on the areas where the model's predictions are the least accurate.\n",
    "\n",
    "3. **Ensemble Combination**: After fitting each weak learner, its predictions are combined with the predictions of the previous learners to update the ensemble model. This combination is typically done by adding the predictions of each learner to the predictions of the current ensemble, with each learner's predictions weighted by a small learning rate. The learning rate controls the contribution of each learner to the ensemble, allowing for more stable and controlled updates.\n",
    "\n",
    "4. **Shrinkage and Regularization**: Gradient Boosting often employs shrinkage (or learning rate) and regularization techniques to prevent overfitting and improve generalization. The learning rate scales the contribution of each weak learner, slowing down the learning process and making the model more robust to noise. Regularization techniques, such as tree depth constraints and feature subsampling, help prevent the model from fitting the training data too closely and improve its ability to generalize to unseen data.\n",
    "\n",
    "Overall, the intuition behind Gradient Boosting lies in the iterative process of sequentially learning from the mistakes of the previous models, gradually improving the ensemble's predictive performance, and controlling the complexity of the final model through regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9f042",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcab647",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. Here's how it typically works:\n",
    "\n",
    "1. **Initialization**: Gradient Boosting starts by initializing the ensemble with a simple model, often just a single weak learner, such as a decision tree with limited depth. This initial model serves as the starting point for the iterative process.\n",
    "\n",
    "2. **Sequential Training**: In each iteration, a new weak learner is added to the ensemble. This weak learner is trained on the residuals, which are the differences between the actual target values and the predictions of the current ensemble model. The goal of each new weak learner is to correct the errors made by the current ensemble.\n",
    "\n",
    "3. **Gradient Descent**: When training each new weak learner, Gradient Boosting uses gradient descent to find the optimal parameters. Instead of directly fitting the weak learner to the target values, it fits it to the negative gradient of the loss function with respect to the predictions of the current ensemble. This negative gradient points in the direction of the steepest decrease in the loss function, allowing the new weak learner to focus on the areas where the current ensemble model performs poorly.\n",
    "\n",
    "4. **Combination of Predictions**: After training each new weak learner, its predictions are combined with the predictions of the previous weak learners to update the ensemble model. Typically, the predictions are combined using a weighted sum, where each weak learner's contribution is scaled by a small learning rate. This combination allows the ensemble model to gradually improve and make more accurate predictions.\n",
    "\n",
    "5. **Iteration**: Steps 2-4 are repeated for a predefined number of iterations or until a stopping criterion is met. With each iteration, the ensemble model becomes more complex and better able to capture the underlying patterns in the data.\n",
    "\n",
    "By iteratively adding new weak learners and combining their predictions, Gradient Boosting builds an ensemble model that is capable of making accurate predictions on a variety of tasks. Each weak learner focuses on correcting the errors of the previous ones, leading to a powerful ensemble model that can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fbcf2",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "### algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364803e8",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several key steps:\n",
    "\n",
    "1. **Loss Function**: Define a loss function that measures the difference between the model's predictions and the true target values. Common loss functions for regression tasks include Mean Squared Error (MSE) and Mean Absolute Error (MAE), while for classification tasks, cross-entropy loss is often used.\n",
    "\n",
    "2. **Initialization**: Initialize the ensemble model with a simple model, often just a single weak learner, such as a decision tree with limited depth. This initial model serves as the starting point for the iterative process.\n",
    "\n",
    "3. **Gradient Descent**: Train each new weak learner using gradient descent to minimize the loss function. Instead of directly fitting the weak learner to the target values, fit it to the negative gradient of the loss function with respect to the predictions of the current ensemble model. This negative gradient points in the direction of the steepest decrease in the loss function, allowing the new weak learner to focus on the areas where the current ensemble model performs poorly.\n",
    "\n",
    "4. **Combination of Predictions**: Combine the predictions of the new weak learner with the predictions of the previous weak learners to update the ensemble model. Typically, the predictions are combined using a weighted sum, where each weak learner's contribution is scaled by a small learning rate. This combination allows the ensemble model to gradually improve and make more accurate predictions.\n",
    "\n",
    "5. **Regularization**: Incorporate regularization techniques to prevent overfitting and improve generalization. Techniques such as controlling the complexity of the weak learners (e.g., limiting tree depth) and introducing randomness (e.g., feature subsampling) help prevent the model from fitting the training data too closely and improve its ability to generalize to unseen data.\n",
    "\n",
    "6. **Iteration**: Repeat steps 3-5 for a predefined number of iterations or until a stopping criterion is met. With each iteration, the ensemble model becomes more complex and better able to capture the underlying patterns in the data.\n",
    "\n",
    "By following these steps, Gradient Boosting constructs an ensemble model that is capable of making accurate predictions on a variety of tasks. Each weak learner focuses on correcting the errors of the previous ones, leading to a powerful ensemble model that can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b500d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
